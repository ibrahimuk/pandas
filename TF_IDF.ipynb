{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibrahimuk/pandas/blob/master/TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Travaux pratiques : Développement d'un moteur de recherche avec TF-Idf\n",
        "Dans ce TP, nous allons développer un moteur de recherche basé sur la technique TF-Idf (Term Frequency-Inverse Document Frequency) appliquée aux critiques de films issues de la base de données IMDB. L'objectif principal de ce TP est d'apprendre à prétraiter des données textuelles, à transformer des textes en représentations numériques exploitables et à construire un système de recherche permettant de retrouver des documents pertinents en fonction de requêtes utilisateur.\n",
        "\n",
        "## Objectifs\n",
        "* Prétraitement des Données Textuelles : Apprendre à nettoyer et préparer les données textuelles en éliminant les balises HTML, en convertissant les textes en minuscules, et en appliquant la lemmatisation pour normaliser les mots.\n",
        "* Transformation en Matrice TF-Idf : Utiliser le vectoriseur TF-Idf pour convertir les critiques de films en une matrice numérique où chaque dimension représente l'importance d'un terme dans un document.\n",
        "*Développement d'un moteur de recherche : Implémenter un moteur de recherche simple qui utilise la similarité cosinus pour retrouver les critiques les plus pertinentes par rapport à une requête utilisateur.\n",
        "\n",
        "## Outils Utilisés\n",
        "* Pandas : Pour le chargement et la manipulation des données.\n",
        "* NLTK (Natural Language Toolkit) : Pour les opérations de prétraitement telles que la lemmatisation et l'élimination des stop words.\n",
        "* Scikit-learn : Pour la transformation des textes en matrices TF-Idf et l'implémentation des algorithmes de recherche.\n",
        "\n",
        "## Étapes du TP\n",
        "* Chargement des données : Importer les critiques de films à partir d'un fichier CSV et afficher les premières lignes pour comprendre la structure des données.\n",
        "* Nettoyage et prétraitement : Appliquer des techniques de nettoyage des textes, comme la suppression des balises HTML et la conversion en minuscules. Utiliser NLTK pour lemmatiser les mots, ce qui permet de réduire les variations grammaticales.\n",
        "* Transformation TF-Idf : Initialiser un TfidfVectorizer pour convertir les critiques en une matrice TF-Idf. Cette matrice permettra de quantifier l'importance des termes dans chaque critique.\n",
        "* Développement du moteur de recherche : Implémenter une fonction de recherche qui utilise la similarité cosinus pour mesurer la pertinence des critiques par rapport à une requête donnée. Afficher les résultats de recherche de manière lisible.\n",
        "* Évaluation et discussion : Tester le moteur de recherche avec différentes requêtes, analyser les résultats obtenus, et discuter des améliorations possibles pour optimiser les performances du système."
      ],
      "metadata": {
        "id": "_MwHD4bIq-ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# Importation de la base de donnée à partir local\n",
        "# import io\n",
        "# df = pd.read_csv(io.BytesIO(uploaded['IMDB.csv']))\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "metadata": {
        "id": "ELdxbko0SVMA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement des données"
      ],
      "metadata": {
        "id": "o9AweWwusBZU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xVYDQ9ykhEWC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "c60f693a-5c8f-4653-fa27-0cf26a23590a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'IMDB.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e339c05984f8>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Charger le dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'IMDB.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'IMDB.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Charger le dataset\n",
        "df = pd.read_csv('IMDB.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimer la première ligne du dataframe\n",
        "print(df['review'].loc[0])"
      ],
      "metadata": {
        "id": "jOQyY4YEjfW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "id": "Kmi5Dho6Wilv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prétraitement des données\n",
        "\n",
        "Cette section du code se concentre sur le prétraitement des critiques de films afin de les préparer pour une analyse ultérieure. Le prétraitement est une étape cruciale dans le traitement du langage naturel (NLP) car il permet de nettoyer et de normaliser les données textuelles.\n",
        "\n",
        "1. **Importation des Bibliothèques NLTK :**\n",
        "\n",
        "* `nltk` est importé avec des modules spécifiques tels que stopwords et wordnet pour le traitement linguistique.\n",
        "* `WordNetLemmatizer` est importé pour effectuer la lemmatisation, une technique qui réduit les mots à leur forme de base.\n",
        "\n",
        "2. **Nettoyage des Données :**\n",
        "\n",
        "Les critiques de films, stockées dans la colonne review du dataframe df, sont nettoyées en supprimant les balises HTML (`<br />`) et en convertissant tout le texte en minuscules. Cela permet d'uniformiser les données et de faciliter l'analyse.\n",
        "\n",
        "3. **Définition de la Fonction de Lemmatisation :**\n",
        "\n",
        "La fonction `lemmatize_text` prend un texte en entrée, le divise en mots (`split`), et lemmatise chaque mot en utilisant `WordNetLemmatizer`, en considérant chaque mot comme un verbe (`pos=wordnet.VERB`), et en excluant les stop words.\n",
        "\n",
        "4. **Application de la Lemmatisation :**\n",
        "\n",
        "La fonction `lemmatize_text` est appliquée à chaque critique de film dans la colonne `review` du dataframe `df`."
      ],
      "metadata": {
        "id": "xA6r5Dt-juOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "sAp5pv5qYxFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# Nettoyer les données : suppression des balises HTML et conversion en minuscules\n",
        "df['review'] = df['review'].str.replace('<br />', '')  # Suppression des balises HTML\n",
        "df['review'] = df['review'].str.lower()  # Conversion en minuscules des données\n",
        "\n",
        "# Initialiser le lemmatiseur WordNet\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))  # Chargement des stop words en anglais\n",
        "\n",
        "# Fonction de lemmatisation\n",
        "def lemmatize_text(text):\n",
        "  ############## Code ############\n",
        "\n",
        "    # Create a lemmatizer object\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Lemmatize each word\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Join the lemmatized words into a lemmatized text string\n",
        "    ################################\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "# Appliquer la lemmatisation\n",
        "df['review'] = df['review'].apply(lemmatize_text)\n",
        "\n",
        "# Afficher la première ligne du dataset prétraité\n",
        "print(df['review'].loc[0])"
      ],
      "metadata": {
        "id": "oFJ_xvwSjztO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformation en matrice TF-IDF\n",
        "\n",
        "Cette section du code concerne la transformation des critiques de films prétraitées en une matrice TF-IDF (Term Frequency-Inverse Document Frequency).\n",
        "\n",
        "1. **Importation du Module TfidfVectorizer de scikit-learn :**\n",
        "\n",
        "Le `TfidfVectorizer` de la bibliothèque `scikit-learn` est utilisé pour convertir les textes en une matrice TF-IDF. Cette méthode permet de quantifier l'importance des termes dans les documents en tenant compte de leur fréquence dans les documents et de leur rareté dans l'ensemble des documents.\n",
        "\n",
        "2. **Initialisation du Vectorizer :**\n",
        "\n",
        "Un objet `TfidfVectorizer` est initialisé avec les paramètres suivants :\n",
        "* `max_features=1000` : Limite le nombre de termes à 1000, en se concentrant sur les termes les plus fréquents.\n",
        "* `stop_words='english'` : Exclut les stop words anglais, qui sont des mots courants n'apportant pas beaucoup de sens (comme \"the\", \"is\", etc.).\n",
        "\n",
        "3. **Transformation des données :**\n",
        "\n",
        "Le `fit_transform` est appliqué sur les critiques de films prétraitées stockées dans `df['review']`. Cette méthode effectue typiquement deux opérations :\n",
        "* `fit` : Apprend le vocabulaire du texte et calcule les statistiques TF-IDF pour chaque terme.\n",
        "* `transform` : Convertit les critiques en vecteurs TF-IDF en utilisant les statistiques apprises."
      ],
      "metadata": {
        "id": "DvRNpWUYu3yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english' )\n",
        "\n",
        "X = vectorizer.fit_transform(df['review'])\n",
        "\n",
        "vectorizer.get_feature_names_out()\n",
        "\n",
        "print(X.shape)\n",
        "\n",
        "(4, 9)\n",
        "\n",
        "# Initialiser le vectorizer\n",
        "# def lemmatize_text(text):\n",
        "  ############## Code ############\n",
        "\n",
        "\n",
        "  ################################\n",
        "\n",
        "# Ajuster sur les données et transformer les données\n",
        "# def lemmatize_text(text):\n",
        "  ############## Code ############\n",
        "\n",
        "\n",
        "  ################################\n",
        "print(\"Shape of TF-IDF matrix:\", X.shape)\n"
      ],
      "metadata": {
        "id": "grgvV1P6n843"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0].toarray()"
      ],
      "metadata": {
        "id": "fULF6oZdoC8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implémentation du moteur de Recherche\n",
        "Cette section du code implémente un moteur de recherche qui permet de retrouver les critiques de films les plus pertinentes en fonction d'une requête utilisateur. Le moteur de recherche utilise la similarité cosinus pour mesurer la pertinence des critiques par rapport à la requête.\n",
        "\n",
        "1. **Importation des bibliothèques nécessaires :**\n",
        "\n",
        "* `numpy` pour la manipulation des tableaux et le tri des indices.\n",
        "* `cosine_similarity` de `sklearn.metrics.pairwise` pour calculer la similarité entre les vecteurs TF-IDF.\n",
        "\n",
        "2. **Définition de la fonction search :**\n",
        "\n",
        "La fonction `search` prend une requête utilisateur, le vectoriseur TF-IDF, la matrice TF-IDF des critiques de films, le dataframe contenant les critiques, et le nombre de résultats à retourner (`top_n`).\n",
        "\n",
        "3. **Transformation de la Requête en Vecteur TF-IDF :**\n",
        "\n",
        "La requête utilisateur est transformée en vecteur TF-IDF en utilisant le vectoriseur initialisé précédemment.\n",
        "\n",
        "4. **Calcul de la similarité cosinus :**\n",
        "\n",
        "La similarité cosinus est calculée entre le vecteur TF-IDF de la requête et la matrice TF-IDF des critiques. La similarité cosinus mesure la proximité entre deux vecteurs dans un espace multidimensionnel.\n",
        "\n",
        "5. **Tri des critiques par similarité :**\n",
        "\n",
        "* Les indices des critiques sont triés en ordre décroissant de similarité.\n",
        "* Les indices des `top_n` critiques les plus similaires sont sélectionnés.\n",
        "* La fonction retourne les indices des critiques les plus similaires et leurs scores de similarité.\n"
      ],
      "metadata": {
        "id": "BgWXT94owLxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def search(query, vectorizer, tfidf_matrix, df, top_n=5):\n",
        "    # Transformer la requête en vecteur TF-Idf\n",
        "    query_vec = vectorizer.transform([query])\n",
        "\n",
        "    # Calculer la similarité cosinus entre la requête et les critiques\n",
        "    similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
        "\n",
        "    # Trouver les indices des critiques les plus similaires\n",
        "    indices = np.argsort(similarities)[::-1][:top_n]\n",
        "\n",
        "    # Retourner les critiques les plus similaires\n",
        "    return indices, similarities[indices]\n",
        "\n",
        "# Exemple d'utilisation\n",
        "query = \"great movie with excellent acting\"\n",
        "indices, similarities = search(query, vectorizer, X, df)\n",
        "\n",
        "# Afficher les résultats\n",
        "for idx, similarity in zip(indices, similarities):\n",
        "    print(f\"Similarity: {similarity:.4f}\")\n",
        "    print(f\"Review: {df.iloc[idx]['review']}\\n\")\n",
        "    print(f\"Sentiment: {df.iloc[idx]['sentiment']}\\n\")\n",
        "    print(\"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "JuauqLnKo6yE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}